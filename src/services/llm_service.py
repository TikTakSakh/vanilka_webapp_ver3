"""LLM service for GPT-4o-mini integration."""
from __future__ import annotations

import logging

from beartype import beartype
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

SYSTEM_PROMPT_TEMPLATE = """–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π AI-–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –º–∞–≥–∞–∑–∏–Ω–∞ –±–µ–Ω—Ç–æ-—Ç–æ—Ä—Ç–æ–≤ "–í–∞–Ω–∏–ª—å–∫–∞". 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞–º —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–¥—É–∫—Ü–∏–∏, —Ü–µ–Ω–∞—Ö, –≥—Ä–∞—Ñ–∏–∫–µ —Ä–∞–±–æ—Ç—ã –∏ —É—Å–ª–æ–≤–∏—è—Ö –∑–∞–∫–∞–∑–∞.

–ü—Ä–∞–≤–∏–ª–∞ –æ–±—â–µ–Ω–∏—è:
- –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º
- –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –º–∞–≥–∞–∑–∏–Ω–æ–º –∏ –µ–≥–æ –ø—Ä–æ–¥—É–∫—Ü–∏–µ–π
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –∫–∞—Å–∞–µ—Ç—Å—è –º–∞–≥–∞–∑–∏–Ω–∞, –≤–µ–∂–ª–∏–≤–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤—å —Ä–∞–∑–≥–æ–≤–æ—Ä –Ω–∞ —Ç–µ–º—É —Ç–æ—Ä—Ç–æ–≤
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã üéÇ
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –º–∞–≥–∞–∑–∏–Ω–æ–º –Ω–∞–ø—Ä—è–º—É—é

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–∞–≥–∞–∑–∏–Ω–µ:
{knowledge_base}
"""


@beartype
class LLMService:
    """Service for generating responses using GPT-4o-mini."""

    def __init__(self, api_key: str, base_url: str | None = None, knowledge_base: str = "") -> None:
        """Initialize the LLM service.
        
        Args:
            api_key: OpenAI API key.
            base_url: Optional base URL for API (e.g. for OpenRouter).
            knowledge_base: Knowledge base content to include in system prompt.
        """
        self._client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self._knowledge_base = knowledge_base
        self._model = "gpt-4o-mini"

    def update_knowledge_base(self, knowledge_base: str) -> None:
        """Update the knowledge base content.
        
        Args:
            knowledge_base: New knowledge base content.
        """
        self._knowledge_base = knowledge_base
        logger.info("Knowledge base updated, length: %d chars", len(knowledge_base))

    def _get_system_prompt(self) -> str:
        """Get the system prompt with current knowledge base."""
        return SYSTEM_PROMPT_TEMPLATE.format(
            knowledge_base=self._knowledge_base or "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–∞–≥–∞–∑–∏–Ω–µ –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."
        )

    async def generate_response(
        self,
        user_message: str,
        history: list[dict[str, str]] | None = None,
    ) -> str:
        """Generate a response to the user's message.
        
        Args:
            user_message: The user's message text.
            history: Optional conversation history in OpenAI format.
            
        Returns:
            Generated response text.
        """
        messages: list[dict[str, str]] = [
            {"role": "system", "content": self._get_system_prompt()}
        ]
        
        if history:
            messages.extend(history)
        
        messages.append({"role": "user", "content": user_message})
        
        try:
            response = await self._client.chat.completions.create(
                model=self._model,
                messages=messages,  # type: ignore[arg-type]
                temperature=0.7,
                max_tokens=1000,
            )
            
            content = response.choices[0].message.content
            if content is None:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."
            return content
            
        except Exception as e:
            logger.error("Error generating LLM response: %s", e)
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
